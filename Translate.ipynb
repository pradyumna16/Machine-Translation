{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from scipy.spatial import distance\n",
    "import nltk as NLTK\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs (english)\n",
    "en_vec_file = './en.vec'\n",
    "en_sent_file =  './demo.500.en.txt' ##    './sent.en.txt'   (if u want to test on 100 samples)\n",
    "vocab_en = []\n",
    "word_vec_en = {}\n",
    "\n",
    "# outputs (german)\n",
    "de_vocab_file = './all_vocab_vectors.de.txt'  ## './de.vocab.txt' (de 100 vocab)\n",
    "de_sent_file = './demo.500.de.txt'      ## './sent.de.txt'\n",
    "vocab_de = []\n",
    "\n",
    "word_vec_dim = 100\n",
    "hidden_unit = 256\n",
    "batch_size = 8\n",
    "epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors_en(vec_file):\n",
    "    pad_vec = []\n",
    "    \n",
    "    ## Padding since all sentences are not of equal length \n",
    "    vocab_en.append('<p>')\n",
    "\n",
    "    for i in range(word_vec_dim):\n",
    "        pad_vec.append(0.0001)\n",
    "\n",
    "    with open(vec_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            arr = line.strip().lower().replace('\\n', '').split(' ')\n",
    "            vec = []\n",
    "            for i in range(1, len(arr)):\n",
    "                vec.append(arr[i])\n",
    "            word_vec_en[arr[0]] = vec\n",
    "            vocab_en.append(arr[0])\n",
    "    f.close()\n",
    "    \n",
    "    word_vec_en['<p>'] = pad_vec\n",
    "    print(len(word_vec_en), 'vectors loaded ...')\n",
    "\n",
    "\n",
    "def load_vocab_de(vocab_file):\n",
    "    fin = open(vocab_file, 'r', encoding='utf-8')\n",
    "    vocab_de.append('<p>')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        vocab_de.append(line)\n",
    "    fin.close()\n",
    "\n",
    "def init():\n",
    "    load_vectors_en(en_vec_file)\n",
    "    load_vocab_de(de_vocab_file)\n",
    "\n",
    "def make_one_hot(idx, y_list):\n",
    "    one_hot_seq = np.zeros((1, len(y_list)))\n",
    "    np.put(one_hot_seq, idx, 1)\n",
    "    return one_hot_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_xy(input_en, output_de):\n",
    "\n",
    "    # x - english\n",
    "    x_max_len = 0\n",
    "    fin = open(input_en, 'r', encoding='utf-8')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        arr = line.split(' ')\n",
    "        if len(arr) > x_max_len:\n",
    "            x_max_len = len(arr)\n",
    "    fin.close()\n",
    "    print('x_max_len', x_max_len)\n",
    "    \n",
    "    x = []\n",
    "    fin = open(input_en, 'r', encoding='utf-8')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        arr = line.split(' ')\n",
    "        temp_x = []\n",
    "        \n",
    "        for w in arr:\n",
    "            if w in vocab_en:\n",
    "                temp_x.append(vocab_en.index(w))\n",
    "        r = x_max_len - len(temp_x)\n",
    "        for i in range(r):\n",
    "            temp_x.append(vocab_en.index('<p>'))\n",
    "        x.append(temp_x)\n",
    "    fin.close()\n",
    "\n",
    "    x = pad_sequences(x, maxlen=x_max_len, dtype='int32', padding='post', truncating='pre', value=0.0)\n",
    "    print('x-shape', x.shape)\n",
    "\n",
    "    # y - output\n",
    "    y_max_len = 0\n",
    "    fin = open(output_de, 'r', encoding='utf-8')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        arr = line.split(' ')\n",
    "        \n",
    "        if len(arr) > y_max_len:\n",
    "            y_max_len = len(arr)\n",
    "    fin.close()\n",
    "    print('y_max_len', y_max_len)\n",
    "    \n",
    "    \n",
    "    y = []\n",
    "    fin = open(output_de, 'r', encoding='utf-8')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        arr = line.split(' ')\n",
    "        temp_y = []\n",
    "        \n",
    "        for w in arr:\n",
    "            if w in vocab_de:\n",
    "                temp_y.append(make_one_hot(vocab_de.index(w), vocab_de))\n",
    "        r = x_max_len - len(temp_y)\n",
    "        for i in range(r):\n",
    "            temp_y.append(make_one_hot(vocab_de.index('<p>'), vocab_de))\n",
    "        y.append(temp_y)\n",
    "\n",
    "    fin.close()\n",
    "    y = np.reshape(y, (len(y), x_max_len, len(vocab_de)))\n",
    "    print('y-shape', y.shape)\n",
    "\n",
    "    return x, x_max_len, y, x_max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_zhou16_atten_vec(inputs):\n",
    "    input_last_dim = int(inputs.shape[2])\n",
    "    m = Activation('tanh')(inputs)\n",
    "    print('M', m._keras_shape)\n",
    "    \n",
    "    alpha = Dense(1, activation='softmax')(m)\n",
    "    print('alpha', alpha._keras_shape)\n",
    "    \n",
    "    input_t = Permute((2, 1))(inputs)\n",
    "    alpha_t = Permute((2, 1))(alpha)\n",
    "    \n",
    "    r_nc = merge([alpha_t, input_t], mode='dot')\n",
    "    print('r', r_nc._keras_shape)\n",
    "    \n",
    "    h_star = Activation('tanh')(r_nc)\n",
    "    print('h_star', h_star._keras_shape)\n",
    "    \n",
    "    attention = Reshape((input_last_dim,), input_shape=(None, 1, input_last_dim))(h_star)\n",
    "    print('attention', attention._keras_shape)\n",
    "    return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_net(x_len, y_len):\n",
    "    input_en_word_ids = Input(shape=(x_len,), dtype='int32', name='input_en_idx')\n",
    "\n",
    "    embedding_matrix = []\n",
    "    \n",
    "    for i in range(len(vocab_en)):\n",
    "        embedding_matrix.append(np.asarray(word_vec_en.get(vocab_en[i]), dtype='float32'))\n",
    "    \n",
    "    embedding_matrix = np.reshape(embedding_matrix, (len(embedding_matrix), word_vec_dim))\n",
    "    \n",
    "    embedded_english_sent = Embedding(input_dim=len(embedding_matrix), output_dim=word_vec_dim, input_length=x_len,\n",
    "                               weights=[embedding_matrix], trainable=True)\n",
    "\n",
    "    x = embedded_english_sent(input_en_word_ids)\n",
    "    \n",
    "    x_dropped = Dropout(0.5)(x)\n",
    "    brnn_output = Bidirectional(LSTM(hidden_unit, return_sequences=True), input_shape=(x_len, word_vec_dim), merge_mode='ave')(x_dropped)\n",
    "    attention_vec = create_zhou16_atten_vec(brnn_output)\n",
    "    repeat_brnn = RepeatVector(x_len)(attention_vec)\n",
    "    x = concatenate([brnn_output,repeat_brnn])\n",
    "    \n",
    "    lstm_out = LSTM(hidden_unit, return_sequences=True, input_shape=(x_len, x[2]))(x)\n",
    "    output_de_ids = TimeDistributed(Dense(len(vocab_de), activation='softmax'), input_shape=(x_len, lstm_out[2]), name='output_output_idx')(lstm_out)\n",
    "\n",
    "    opts = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model = Model(inputs=[input_en_word_ids], outputs=[output_de_ids])\n",
    "    model.compile(optimizer=opts, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    print('model-input ', model.input_shape)\n",
    "    print('model-params ', model.count_params())\n",
    "    print('model-output ', model.output_shape)\n",
    "    print('model-summary ')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29229 vectors loaded ...\n",
      "x_max_len 86\n",
      "x-shape (500, 86)\n",
      "y_max_len 88\n",
      "y-shape (500, 86, 29426)\n",
      "M (None, 86, 256)\n",
      "alpha (None, 86, 1)\n",
      "r (None, 1, 256)\n",
      "h_star (None, 1, 256)\n",
      "attention (None, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prady\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Users\\Prady\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\legacy\\layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-input  (None, 86)\n",
      "model-params  12031131\n",
      "model-output  (None, 86, 29426)\n",
      "model-summary \n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_en_idx (InputLayer)        (None, 86)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 86, 100)       2949800                                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 86, 100)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 86, 256)       731136                                       \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 86, 256)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 86, 1)         257                                          \n",
      "____________________________________________________________________________________________________\n",
      "permute_2 (Permute)              (None, 1, 86)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "permute_1 (Permute)              (None, 256, 86)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 1, 256)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 1, 256)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 86, 256)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 86, 512)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 86, 256)       787456                                       \n",
      "____________________________________________________________________________________________________\n",
      "output_output_idx (TimeDistribut (None, 86, 29426)     7562482                                      \n",
      "====================================================================================================\n",
      "Total params: 12,031,131.0\n",
      "Trainable params: 12,031,131.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/500\n",
      "450/450 [==============================] - 48s - loss: 5.8584e-06 - acc: 1.0000 - val_loss: 6.8083e-07 - val_acc: 1.0000\n",
      "Epoch 2/500\n",
      "450/450 [==============================] - 38s - loss: 5.4823e-07 - acc: 1.0000 - val_loss: 4.4884e-07 - val_acc: 1.0000\n",
      "Epoch 3/500\n",
      "450/450 [==============================] - 38s - loss: 3.8887e-07 - acc: 1.0000 - val_loss: 3.3815e-07 - val_acc: 1.0000\n",
      "Epoch 4/500\n",
      "440/450 [============================>.] - ETA: 0s - loss: 3.0570e-07 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "def translate(model, x_max_len, sents):\n",
    "    x = []\n",
    "    for s in sents:\n",
    "        arr = s.split(' ')\n",
    "        temp_x = []\n",
    "        \n",
    "        for w in arr:\n",
    "            temp_x.append(vocab_en.index(w))\n",
    "        r = x_max_len -len(temp_x)\n",
    "        for i in range(r):\n",
    "            temp_x.append(vocab_en.index('<p>'))\n",
    "        x.append(temp_x)\n",
    "\n",
    "    output_de_sents = model.predict(x=x, batch_size=4, verbose=1)\n",
    "\n",
    "    for i in range(len(output_de_sents)):\n",
    "        de_s = output_de_sents[i]\n",
    "        strs = ''\n",
    "        for j in range(len(de_s)):\n",
    "            w = vocab_de[np.argmax(de_s[j], axis=0)]\n",
    "            if w != '<p>':\n",
    "                strs += w+ ' '\n",
    "        strs = strs.strip()\n",
    "        print(i, strs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_file = Path('en-de-translation.h5')\n",
    "\n",
    "    # training\n",
    "    init()\n",
    "    x, x_l, y, y_l = make_xy(input_en=en_sent_file, output_de=de_sent_file)\n",
    "    \n",
    "    ### comment next 3 lines if already trained once- to avoid repeated trainings\n",
    "    model = init_net(x_l, y_l)\n",
    "    \n",
    "    if model_file.is_file():\n",
    "        model.load_weights('en-de-translation.h5')\n",
    "    \n",
    "    model.fit(x=x, y=y, batch_size=batch_size, epochs=epoch, validation_split=0.1)\n",
    "    model.save_weights(model_file)\n",
    "\n",
    "    # testing\n",
    "    model = init_net(x_len=x_l, y_len=y_l)\n",
    "    model.load_weights('en-de-translation.h5')\n",
    "    sent_arr = ['i declare resumed the session of the european parliament ad@@ jour@@ ned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant fes@@ tive period .',\n",
    "                'i should now like to comment on the issue itself .',\n",
    "                'i should now like to comment on the issue itself .',\n",
    "                'i should now like to comment on the issue itself .']\n",
    "\n",
    "    translate(model, x_max_len=x_l, sents=sent_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow-gpu]",
   "language": "python",
   "name": "Python [tensorflow-gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
