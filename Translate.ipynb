{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from scipy.spatial import distance\n",
    "import nltk as NLTK\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs (english)\n",
    "en_vec_file = './en.vec'\n",
    "en_sent_file = './sent.en.txt' ## demo.100.en.txt (if u want to test on 100 samples)\n",
    "vocab_en = []\n",
    "word_vec_en = {}\n",
    "\n",
    "# outputs (german)\n",
    "de_vocab_file = './de.vocab.txt' ## demo-100-vocab.txt (de 100 vocab)\n",
    "de_sent_file = './sent.de.txt' ## demo.100.de.txt\n",
    "vocab_de = []\n",
    "\n",
    "word_vec_dim = 100\n",
    "hidden_unit = 256\n",
    "batch_size = 64\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors_en(vec_file):\n",
    "    pad_vec = []\n",
    "    \n",
    "    ## Padding since all sentences are not of equal length \n",
    "    vocab_en.append('<p>')\n",
    "\n",
    "    for i in range(word_vec_dim):\n",
    "        pad_vec.append(0.0)\n",
    "\n",
    "    with open(vec_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            arr = line.strip().lower().replace('\\n', '').split(' ')\n",
    "            vec = []\n",
    "            for i in range(1, len(arr)):\n",
    "                vec.append(arr[i])\n",
    "            word_vec_en[arr[0]] = vec\n",
    "            vocab_en.append(arr[0])\n",
    "    f.close()\n",
    "    \n",
    "    word_vec_en['<p>'] = pad_vec\n",
    "    print(len(word_vec_en), 'vectors loaded ...')\n",
    "\n",
    "\n",
    "def load_vocab_de(vocab_file):\n",
    "    fin = open(vocab_file, 'r', encoding='utf-8')\n",
    "    vocab_de.append('<p>')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        vocab_de.append(line)\n",
    "    fin.close()\n",
    "\n",
    "def init():\n",
    "    load_vectors_en(en_vec_file)\n",
    "    load_vocab_de(de_vocab_file)\n",
    "\n",
    "def make_one_hot(idx, y_list):\n",
    "    one_hot_seq = np.zeros((1, len(y_list)))\n",
    "    np.put(one_hot_seq, idx, 1)\n",
    "    return one_hot_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_xy(input_en, output_de):\n",
    "\n",
    "    # x - english\n",
    "    x_max_len = 0\n",
    "    fin = open(input_en, 'r', encoding='utf-8')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        arr = line.split(' ')\n",
    "        if len(arr) > x_max_len:\n",
    "            x_max_len = len(arr)\n",
    "    fin.close()\n",
    "    print('x_max_len', x_max_len)\n",
    "    \n",
    "    x = []\n",
    "    fin = open(input_en, 'r', encoding='utf-8')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        arr = line.split(' ')\n",
    "        temp_x = []\n",
    "        for w in arr:\n",
    "            if w in vocab_en:\n",
    "                temp_x.append(vocab_en.index(w))\n",
    "        r = x_max_len - len(temp_x)\n",
    "        for i in range(r):\n",
    "            temp_x.append(vocab_en.index('<p>'))\n",
    "        x.append(temp_x)\n",
    "    fin.close()\n",
    "\n",
    "    x = pad_sequences(x, maxlen=x_max_len, dtype='int32', padding='post', truncating='pre', value=0.0)\n",
    "    print('x-shape', x.shape)\n",
    "\n",
    "    # y - output\n",
    "    y_max_len = 0\n",
    "    fin = open(output_de, 'r', encoding='utf-8')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        arr = line.split(' ')\n",
    "        if len(arr) > y_max_len:\n",
    "            y_max_len = len(arr)\n",
    "    fin.close()\n",
    "    print('y_max_len', y_max_len)\n",
    "    y = []\n",
    "    fin = open(output_de, 'r', encoding='utf-8')\n",
    "    for line in fin:\n",
    "        line = line.strip().lower().replace('\\n', '').strip()\n",
    "        arr = line.split(' ')\n",
    "        temp_y = []\n",
    "        for w in arr:\n",
    "            if w in vocab_de:\n",
    "                temp_y.append(make_one_hot(vocab_de.index(w), vocab_de))\n",
    "        r = y_max_len - len(temp_y)\n",
    "        for i in range(r):\n",
    "            temp_y.append(make_one_hot(vocab_de.index('<p>'), vocab_de))\n",
    "        y.append(temp_y)\n",
    "\n",
    "    fin.close()\n",
    "    y = np.reshape(y, (len(y), y_max_len, len(vocab_de)))\n",
    "    print('y-shape', y.shape)\n",
    "\n",
    "    return x, x_max_len, y, y_max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_net(x_len, y_len):\n",
    "    input_en_word_ids = Input(shape=(x_len,), dtype='int32', name='input_en_idx')\n",
    "\n",
    "    embedding_matrix = []\n",
    "    for i in range(len(vocab_en)):\n",
    "        embedding_matrix.append(np.asarray(word_vec_en.get(vocab_en[i]), dtype='float32'))\n",
    "    embedding_matrix = np.reshape(embedding_matrix, (len(embedding_matrix), word_vec_dim))\n",
    "    embedded_english_sent = Embedding(input_dim=len(embedding_matrix), output_dim=word_vec_dim, input_length=x_len,\n",
    "                               weights=[embedding_matrix], trainable=True, mask_zero=True)\n",
    "\n",
    "    x = embedded_english_sent(input_en_word_ids)\n",
    "    x_dropped = Dropout(0.5)(x)\n",
    "    brnn_output = Bidirectional(LSTM(hidden_unit, return_sequences=False), input_shape=(x_len, word_vec_dim), merge_mode='ave')(x_dropped)\n",
    "    repeat_brnn = RepeatVector(y_len)(brnn_output)\n",
    "    lstm_out = LSTM(hidden_unit, return_sequences=True, input_shape=(y_len, hidden_unit))(repeat_brnn)\n",
    "    output_de_ids = TimeDistributed(Dense(len(vocab_de), activation='softmax'), input_shape=(y_len, lstm_out[2]), name='output_output_idx')(lstm_out)\n",
    "\n",
    "    opts = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model = Model(inputs=[input_en_word_ids], outputs=[output_de_ids])\n",
    "    model.compile(optimizer=opts, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print('model-input ', model.input_shape)\n",
    "    print('model-params ', model.count_params())\n",
    "    print('model-output ', model.output_shape)\n",
    "    print('model-summary ')\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29229 vectors loaded ...\n",
      "x_max_len 484\n"
     ]
    }
   ],
   "source": [
    "def translate(model, x_max_len, sents):\n",
    "    x = []\n",
    "    for s in sents:\n",
    "        arr = s.split(' ')\n",
    "        temp_x = []\n",
    "        for w in arr:\n",
    "            temp_x.append(vocab_en.index(w))\n",
    "        r = x_max_len -len(temp_x)\n",
    "        for i in range(r):\n",
    "            temp_x.append(vocab_en.index('<p>'))\n",
    "        x.append(temp_x)\n",
    "\n",
    "    output_de_sents = model.predict(x=x, batch_size=4, verbose=1)\n",
    "\n",
    "    for i in range(len(output_de_sents)):\n",
    "        de_s = output_de_sents[i]\n",
    "        strs = ''\n",
    "        for j in range(len(de_s)):\n",
    "            w = vocab_de[np.argmax(de_s[j], axis=0)]\n",
    "            if w != '<p>':\n",
    "                strs += w+ ' '\n",
    "        strs = strs.strip()\n",
    "        print(i, strs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # training\n",
    "    init()\n",
    "    x, x_l, y, y_l = make_xy(input_en=en_sent_file, output_de=de_sent_file)\n",
    "    \n",
    "    ###\n",
    "    model = init_net(x_l, y_l)\n",
    "    model.fit(x=x, y=y, batch_size=batch_size, epochs=epoch, validation_split=0.1)\n",
    "    model.save_weights('en-de-translation.h5')\n",
    "\n",
    "    # testing\n",
    "    model = init_net(x_len=x_l, y_len=y_l)\n",
    "    model.load_weights('en-de-translation.h5')\n",
    "    sent_arr = ['i declare resumed the session of the european parliament ad@@ jour@@ ned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant fes@@ tive period .',\n",
    "                'i should now like to comment on the issue itself .',\n",
    "                'i should now like to comment on the issue itself .',\n",
    "                'i should now like to comment on the issue itself .']\n",
    "\n",
    "    translate(model, x_max_len=x_l, sents=sent_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow-gpu]",
   "language": "python",
   "name": "Python [tensorflow-gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
